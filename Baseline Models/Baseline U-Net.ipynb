{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12807,
     "status": "ok",
     "timestamp": 1552773429425,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "ZZG4BqkENEyd",
    "outputId": "0b8bd808-eba5-4de2-9c01-64058b721fba"
   },
   "outputs": [],
   "source": [
    "# Taken from\n",
    "# https://stackoverflow.com/questions/48750199/google-colaboratory-misleading-information-about-its-gpu-only-5-ram-available\n",
    "# memory footprint support libraries/code\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "# Colab only provides one GPU and it is not always guaranteed\n",
    "gpu = GPUs[0]\n",
    "def printm():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(\"RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2475,
     "status": "ok",
     "timestamp": 1552773429426,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "BBvIvBoyg68g",
    "outputId": "64e90310-ddee-4cb4-a672-41a831cc0503"
   },
   "outputs": [],
   "source": [
    "printm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4124,
     "status": "ok",
     "timestamp": 1552773455053,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "yV1m-9ZGuKGj",
    "outputId": "32fde251-f5a5-4b68-9b12-9e42974bffae"
   },
   "outputs": [],
   "source": [
    "# Clone repo\n",
    "!git clone https://github.com/MatchLab-Imperial/keras_triplet_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 978,
     "status": "ok",
     "timestamp": 1552773459405,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "pyZSqhZ5LACT",
    "outputId": "ce0a5f2e-73e9-43f0-f80b-41950680731a"
   },
   "outputs": [],
   "source": [
    "# Change directory\n",
    "%cd keras_triplet_descriptor    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 190209,
     "status": "ok",
     "timestamp": 1552773652267,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "307CBCL-FjX4",
    "outputId": "9b10a27f-fe9c-4492-a089-784596f08ac0"
   },
   "outputs": [],
   "source": [
    "# Download data\n",
    "!wget -O hpatches_data.zip https://imperialcollegelondon.box.com/shared/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 249292,
     "status": "ok",
     "timestamp": 1552773724185,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "36mBTFvPCxY9",
    "outputId": "b0b44eeb-fe9c-4c11-e2e1-8a67725abd68"
   },
   "outputs": [],
   "source": [
    "# Extract data\n",
    "!unzip -q ./hpatches_data.zip\n",
    "!rm ./hpatches_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0KYfe-at9KN"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda, Reshape\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Conv2DTranspose\n",
    "from keras.layers import Input, UpSampling2D, concatenate  \n",
    "\n",
    "from read_data import HPatches, DataGeneratorDesc, hpatches_sequence_folder, DenoiseHPatches, tps\n",
    "from utils import generate_desc_csv, plot_denoise, plot_triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NXL31ez-AT5h"
   },
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ABKDHB9RApZk"
   },
   "outputs": [],
   "source": [
    "hpatches_dir = './hpatches'\n",
    "splits_path = 'splits.json'\n",
    "\n",
    "splits_json = json.load(open(splits_path, 'rb'))\n",
    "split = splits_json['a']\n",
    "\n",
    "train_fnames = split['train']\n",
    "test_fnames = split['test']\n",
    "\n",
    "seqs = glob.glob(hpatches_dir+'/*')\n",
    "seqs = [os.path.abspath(p) for p in seqs]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs_train = list(filter(lambda x: x.split('\\\\')[-1] in train_fnames, seqs)) \n",
    "seqs_test = list(filter(lambda x: x.split('\\\\')[-1] in split['test'], seqs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(seqs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(seqs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qeWik0vMEtuC"
   },
   "source": [
    "## Models and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6QbkHnbuIUD"
   },
   "outputs": [],
   "source": [
    "def get_denoise_model(shape, do=0):\n",
    "  \n",
    "    inputs = Input(shape)\n",
    "\n",
    "    ## Encoder starts\n",
    "    conv1 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    ## Bottleneck\n",
    "    conv2 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "\n",
    "    ## Now the decoder starts\n",
    "    up3 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv2))\n",
    "    merge3 = concatenate([conv1,up3], axis = -1)\n",
    "    conv3 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge3)\n",
    "\n",
    "    conv4 = Conv2D(1, 3,  padding = 'same')(conv3)\n",
    "\n",
    "    shallow_net = Model(inputs = inputs, outputs = conv4)\n",
    "\n",
    "    return shallow_net\n",
    "\n",
    "\n",
    "def get_descriptor_model(shape):\n",
    "  \n",
    "    '''Architecture copies HardNet architecture'''\n",
    "\n",
    "    init_weights = keras.initializers.he_normal()\n",
    "\n",
    "    descriptor_model = Sequential()\n",
    "    descriptor_model.add(Conv2D(32, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights))\n",
    "    descriptor_model.add(BatchNormalization(axis = -1))\n",
    "    descriptor_model.add(Activation('relu'))\n",
    "\n",
    "    descriptor_model.add(Conv2D(32, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
    "    descriptor_model.add(BatchNormalization(axis = -1))\n",
    "    descriptor_model.add(Activation('relu'))\n",
    "\n",
    "    descriptor_model.add(Conv2D(64, 3, padding='same', strides=2, use_bias = True, kernel_initializer=init_weights))\n",
    "    descriptor_model.add(BatchNormalization(axis = -1))\n",
    "    descriptor_model.add(Activation('relu'))\n",
    "\n",
    "    descriptor_model.add(Conv2D(64, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
    "    descriptor_model.add(BatchNormalization(axis = -1))\n",
    "    descriptor_model.add(Activation('relu'))\n",
    "\n",
    "    descriptor_model.add(Conv2D(128, 3, padding='same', strides=2,  use_bias = True, kernel_initializer=init_weights))\n",
    "    descriptor_model.add(BatchNormalization(axis = -1))\n",
    "    descriptor_model.add(Activation('relu'))\n",
    "\n",
    "    descriptor_model.add(Conv2D(128, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
    "    descriptor_model.add(BatchNormalization(axis = -1))\n",
    "    descriptor_model.add(Activation('relu'))\n",
    "    descriptor_model.add(Dropout(0.3))\n",
    "\n",
    "    descriptor_model.add(Conv2D(128, 8, padding='valid', use_bias = True, kernel_initializer=init_weights))\n",
    "\n",
    "    # Final descriptor reshape\n",
    "    descriptor_model.add(Reshape((128,)))\n",
    "\n",
    "    return descriptor_model\n",
    "  \n",
    "\n",
    "def triplet_loss(x):\n",
    "  \n",
    "    output_dim = 128\n",
    "    a, p, n = x\n",
    "    _alpha = 1.0\n",
    "    positive_distance = K.mean(K.square(a - p), axis=-1)\n",
    "    negative_distance = K.mean(K.square(a - n), axis=-1)\n",
    "\n",
    "    return K.expand_dims(K.maximum(0.0, positive_distance - negative_distance + _alpha), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RlS5zcV7EJgp"
   },
   "source": [
    "## Denoising Image Patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 113505,
     "status": "ok",
     "timestamp": 1552773869491,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "m_VPSHmSK0dS",
    "outputId": "50e9d1d1-368e-4ae8-ba54-d3dba09d503b"
   },
   "outputs": [],
   "source": [
    "#denoise_generator = DenoiseHPatches(random.sample(seqs_train, 5), batch_size=50)\n",
    "#denoise_generator_val = DenoiseHPatches(random.sample(seqs_test, 1), batch_size=50)\n",
    "\n",
    "# Uncomment following lines for using all the data to train the denoising model\n",
    "denoise_generator = DenoiseHPatches(seqs_train, batch_size=50)\n",
    "denoise_generator_val = DenoiseHPatches(seqs_test, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vary Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1293,
     "status": "ok",
     "timestamp": 1552773922597,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "-eUSba93Dttj",
    "outputId": "9888dc14-c015-469b-f13c-d1576f667cbb"
   },
   "outputs": [],
   "source": [
    "shape = (32, 32, 1)\n",
    "denoise_model_sgd_lr1 = get_denoise_model(shape)\n",
    "denoise_model_sgd_lr2 = get_denoise_model(shape)\n",
    "denoise_model_sgd_lr3 = get_denoise_model(shape)\n",
    "denoise_model_sgd_lr4 = get_denoise_model(shape)\n",
    "denoise_model_sgd_lr5 = get_denoise_model(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1417,
     "status": "error",
     "timestamp": 1552811688250,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "edwbgE6yKqcD",
    "outputId": "d633c862-209f-4455-85be-4a19155c50f9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgd = keras.optimizers.SGD(lr=0.000001, momentum=0.9, nesterov=True)\n",
    "sgd2 = keras.optimizers.SGD(lr=0.00001, momentum=0.9, nesterov=True)\n",
    "sgd3 = keras.optimizers.SGD(lr=0.0001, momentum=0.9, nesterov=True)\n",
    "sgd4 = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
    "sgd5 = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "\n",
    "\n",
    "\n",
    "denoise_model_sgd_lr1.compile(loss='mean_absolute_error', optimizer=sgd, metrics=['mae'])\n",
    "denoise_model_sgd_lr2.compile(loss='mean_absolute_error', optimizer=sgd2, metrics=['mae'])\n",
    "denoise_model_sgd_lr3.compile(loss='mean_absolute_error', optimizer=sgd3, metrics=['mae'])\n",
    "denoise_model_sgd_lr4.compile(loss='mean_absolute_error', optimizer=sgd4, metrics=['mae'])\n",
    "denoise_model_sgd_lr5.compile(loss='mean_absolute_error', optimizer=sgd5, metrics=['mae'])\n",
    "\n",
    "\n",
    "#epochs = 1\n",
    "### Use a loop to save for each epoch the weights in an external website in\n",
    "### case colab stops. Every time you call fit/fit_generator the weigths are NOT\n",
    "### reset, so e.g. calling 5 times fit(epochs=1) behave as fit(epochs=5)\n",
    "#for e in range(epochs):\n",
    "denoise_history_sgd_lr1 = denoise_model_sgd_lr1.fit_generator(generator=denoise_generator, \n",
    "                                            epochs=5, verbose=1, \n",
    "                                            validation_data=denoise_generator_val)\n",
    "\n",
    "denoise_history_sgd_lr2 = denoise_model_sgd_lr2.fit_generator(generator=denoise_generator, \n",
    "                                            epochs=5, verbose=1, \n",
    "                                            validation_data=denoise_generator_val)\n",
    "\n",
    "denoise_history_sgd_lr3 = denoise_model_sgd_lr3.fit_generator(generator=denoise_generator, \n",
    "                                            epochs=5, verbose=1, \n",
    "                                            validation_data=denoise_generator_val)\n",
    "\n",
    "denoise_history_sgd_lr4 = denoise_model_sgd_lr4.fit_generator(generator=denoise_generator, \n",
    "                                            epochs=5, verbose=1, \n",
    "                                            validation_data=denoise_generator_val)\n",
    "\n",
    "denoise_history_sgd_lr5 = denoise_model_sgd_lr5.fit_generator(generator=denoise_generator, \n",
    "                                            epochs=5, verbose=1, \n",
    "                                            validation_data=denoise_generator_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del denoise_generator\n",
    "del denoise_generator_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XFA_8uN4Eb3B"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, history2, history3, history4, history5, metric = None):\n",
    "  # Plots the loss history of training and validation (if existing)\n",
    "  # and a given metric\n",
    "  \n",
    "  if metric != None:\n",
    "    fig, axes = plt.subplots(2,1, figsize=(8, 10))\n",
    "    axes[0].plot(history.history[metric])\n",
    "    axes[0].plot(history2.history[metric])\n",
    "    axes[0].plot(history3.history[metric])\n",
    "    axes[0].plot(history4.history[metric])\n",
    "    axes[0].plot(history5.history[metric])\n",
    "    \n",
    "    try:\n",
    "      #axes[0].plot(history.history['val_'+metric])\n",
    "      #axes[0].plot(history2.history['val2_'+metric])\n",
    "      #axes[0].plot(history3.history['val3_'+metric])\n",
    "      axes[0].legend(['lr=1e-6', 'lr=1e-5', 'lr=1e-4', 'lr=1e-3', 'lr=1e-2'], loc='best')\n",
    "    except:\n",
    "      pass\n",
    "    axes[0].set_title('MAE Vs. No of Epochs for Various Learning Rates')\n",
    "    axes[0].set_ylabel('Mean Absolute Error')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    axes[1].plot(history.history['loss'])\n",
    "    axes[1].plot(history2.history['loss'])\n",
    "    axes[1].plot(history3.history['loss'])\n",
    "    axes[1].plot(history4.history['loss'])\n",
    "    axes[1].plot(history5.history['loss'])\n",
    "    try:\n",
    "      #axes[1].plot(history.history['val_loss'])\n",
    "      axes[1].legend(['lr=1e-6', 'lr=1e-5', 'lr=1e-4', 'lr=1e-3', 'lr=1e-2'], loc='best')\n",
    "    except:\n",
    "      pass\n",
    "    axes[1].set_title('MAE Vs. No of Epochs for Various Learning Rates')\n",
    "    axes[1].set_ylabel('Mean Absolute Error')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "  else:\n",
    "    plt.plot(history.history['loss'])\n",
    "    try:\n",
    "      plt.plot(history.history['val_loss'])\n",
    "      plt.legend(['Train', 'Val'])\n",
    "    except:\n",
    "      pass\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "  \n",
    "plot_history(denoise_history_sgd_lr1, denoise_history_sgd_lr2, denoise_history_sgd_lr3, denoise_history_sgd_lr4, denoise_history_sgd_lr5, 'mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_val_history(history, history2, history3, history4, history5, metric = None):\n",
    "  # Plots the loss history of training and validation (if existing)\n",
    "  # and a given metric\n",
    "  \n",
    "  if metric != None:\n",
    "    fig, axes = plt.subplots(2,1, figsize=(8, 10))\n",
    "    #axes[0].plot(history.history[metric])\n",
    "    #axes[0].plot(history2.history[metric])\n",
    "    #axes[0].plot(history3.history[metric])\n",
    "    try:\n",
    "      axes[0].plot(history.history['val_'+metric])\n",
    "      axes[0].plot(history2.history['val_'+metric])\n",
    "      axes[0].plot(history3.history['val_'+metric])\n",
    "      axes[0].plot(history4.history['val_'+metric])\n",
    "      axes[0].plot(history5.history['val_'+metric])\n",
    "      axes[0].legend(['lr=1e-6', 'lr=1e-5', 'lr=1e-4', 'lr=1e-3', 'lr=1e-2'], loc='best')\n",
    "    except:\n",
    "      pass\n",
    "    axes[0].set_title('Validation Loss Vs. No of Epochs for for Various Learning Rates')\n",
    "    axes[0].set_ylabel('Validation Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    #axes[1].plot(history.history['loss'])\n",
    "    #axes[1].plot(history2.history['loss'])\n",
    "    #axes[1].plot(history3.history['loss'])\n",
    "    try:\n",
    "      axes[1].plot(history.history['val_loss'])\n",
    "      axes[1].plot(history2.history['val_loss'])\n",
    "      axes[1].plot(history3.history['val_loss'])\n",
    "      axes[1].plot(history4.history['val_loss'])\n",
    "      axes[1].plot(history5.history['val_loss'])\n",
    "      axes[1].legend(['lr=1e-6', 'lr=1e-5', 'lr=1e-4', 'lr=1e-3', 'lr=1e-2'], loc='best')\n",
    "    except:\n",
    "      pass\n",
    "    axes[1].set_title('Validation Loss Vs. No of Epochs for Various Learning Rates')\n",
    "    axes[1].set_ylabel('Validation Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "  else:\n",
    "    plt.plot(history.history['loss'])\n",
    "    try:\n",
    "      plt.plot(history.history['val_loss'])\n",
    "      plt.legend(['Train', 'Val'])\n",
    "    except:\n",
    "      pass\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "  \n",
    "plot_val_history(denoise_history_sgd_lr1, denoise_history_sgd_lr2, denoise_history_sgd_lr3, denoise_history_sgd_lr4, denoise_history_sgd_lr5, 'mean_absolute_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vary Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgd = keras.optimizers.SGD(lr=0.00001, momentum=0.9, nesterov=True)\n",
    "sgd2 = keras.optimizers.SGD(lr=0.00001, momentum=0.8, nesterov=True)\n",
    "sgd3 = keras.optimizers.SGD(lr=0.00001, momentum=0.7, nesterov=True)\n",
    "sgd4 = keras.optimizers.SGD(lr=0.00001, momentum=0.6, nesterov=True)\n",
    "sgd5 = keras.optimizers.SGD(lr=0.00001, momentum=0.5, nesterov=True)\n",
    "\n",
    "\n",
    "\n",
    "denoise_model_sgd_mo1.compile(loss='mean_absolute_error', optimizer=sgd, metrics=['mae'])\n",
    "denoise_model_sgd_mo2.compile(loss='mean_absolute_error', optimizer=sgd2, metrics=['mae'])\n",
    "denoise_model_sgd_mo3.compile(loss='mean_absolute_error', optimizer=sgd3, metrics=['mae'])\n",
    "denoise_model_sgd_mo4.compile(loss='mean_absolute_error', optimizer=sgd4, metrics=['mae'])\n",
    "denoise_model_sgd_mo5.compile(loss='mean_absolute_error', optimizer=sgd5, metrics=['mae'])\n",
    "\n",
    "\n",
    "#epochs = 1\n",
    "### Use a loop to save for each epoch the weights in an external website in\n",
    "### case colab stops. Every time you call fit/fit_generator the weigths are NOT\n",
    "### reset, so e.g. calling 5 times fit(epochs=1) behave as fit(epochs=5)\n",
    "#for e in range(epochs):\n",
    "denoise_history_sgd_mo1 = denoise_model_sgd_mo1.fit_generator(generator=denoise_generator, \n",
    "                                            epochs=5, verbose=1, \n",
    "                                            validation_data=denoise_generator_val)\n",
    "\n",
    "denoise_history_sgd_mo2= denoise_model_sgd_mo2.fit_generator(generator=denoise_generator, \n",
    "                                            epochs=5, verbose=1, \n",
    "                                            validation_data=denoise_generator_val)\n",
    "\n",
    "denoise_history_sgd_mo3 = denoise_model_sgd_mo3.fit_generator(generator=denoise_generator, \n",
    "                                            epochs=5, verbose=1, \n",
    "                                            validation_data=denoise_generator_val)\n",
    "\n",
    "denoise_history_sgd_mo4= denoise_model_sgd_mo4.fit_generator(generator=denoise_generator, \n",
    "                                            epochs=5, verbose=1, \n",
    "                                            validation_data=denoise_generator_val)\n",
    "\n",
    "denoise_history_sgd_mo5 = denoise_model_sgd_mo5.fit_generator(generator=denoise_generator, \n",
    "                                            epochs=5, verbose=1, \n",
    "                                            validation_data=denoise_generator_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_history(history, history2, history3, history4, history5, metric = None):\n",
    "  # Plots the loss history of training and validation (if existing)\n",
    "  # and a given metric\n",
    "  \n",
    "  if metric != None:\n",
    "    fig, axes = plt.subplots(2,1, figsize=(8, 10))\n",
    "    axes[0].plot(history.history[metric])\n",
    "    axes[0].plot(history2.history[metric])\n",
    "    axes[0].plot(history3.history[metric])\n",
    "    axes[0].plot(history4.history[metric])\n",
    "    axes[0].plot(history5.history[metric])\n",
    "    \n",
    "    try:\n",
    "      #axes[0].plot(history.history['val_'+metric])\n",
    "      #axes[0].plot(history2.history['val2_'+metric])\n",
    "      #axes[0].plot(history3.history['val3_'+metric])\n",
    "      axes[0].legend(['0.9', '0.8', '0.7', '0.6', '0.5'], loc='best')\n",
    "    except:\n",
    "      pass\n",
    "    axes[0].set_title('MAE Vs. No of Epochs for Various Momentum Values')\n",
    "    axes[0].set_ylabel('Mean Absolute Error')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    axes[1].plot(history.history['loss'])\n",
    "    axes[1].plot(history2.history['loss'])\n",
    "    axes[1].plot(history3.history['loss'])\n",
    "    axes[1].plot(history4.history['loss'])\n",
    "    axes[1].plot(history5.history['loss'])\n",
    "    try:\n",
    "      #axes[1].plot(history.history['val_loss'])\n",
    "      axes[1].legend(['0.9', '0.8', '0.7', '0.6', '0.5'], loc='best')\n",
    "    except:\n",
    "      pass\n",
    "    axes[1].set_title('MAE Vs. No of Epochs for Various Momentum Values')\n",
    "    axes[1].set_ylabel('Mean Absolute Error')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "  else:\n",
    "    plt.plot(history.history['loss'])\n",
    "    try:\n",
    "      plt.plot(history.history['val_loss'])\n",
    "      plt.legend(['Train', 'Val'])\n",
    "    except:\n",
    "      pass\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "  \n",
    "plot_history(denoise_history_sgd_mo1, denoise_history_sgd_mo2, denoise_history_sgd_mo3, denoise_history_sgd_mo4, denoise_history_sgd_mo5, 'mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_val_history(history, history2, history3, history4, history5, metric = None):\n",
    "  # Plots the loss history of training and validation (if existing)\n",
    "  # and a given metric\n",
    "  \n",
    "  if metric != None:\n",
    "    fig, axes = plt.subplots(2,1, figsize=(8, 10))\n",
    "    #axes[0].plot(history.history[metric])\n",
    "    #axes[0].plot(history2.history[metric])\n",
    "    #axes[0].plot(history3.history[metric])\n",
    "    try:\n",
    "      axes[0].plot(history.history['val_'+metric])\n",
    "      axes[0].plot(history2.history['val_'+metric])\n",
    "      axes[0].plot(history3.history['val_'+metric])\n",
    "      axes[0].plot(history4.history['val_'+metric])\n",
    "      axes[0].plot(history5.history['val_'+metric])\n",
    "      axes[0].legend(['0.9', '0.8', '0.7', '0.6', '0.5'], loc='best')\n",
    "    except:\n",
    "      pass\n",
    "    axes[0].set_title('Validation Loss Vs. No of Epochs for for Various Momentum Values')\n",
    "    axes[0].set_ylabel('Validation Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    #axes[1].plot(history.history['loss'])\n",
    "    #axes[1].plot(history2.history['loss'])\n",
    "    #axes[1].plot(history3.history['loss'])\n",
    "    try:\n",
    "      axes[1].plot(history.history['val_loss'])\n",
    "      axes[1].plot(history2.history['val_loss'])\n",
    "      axes[1].plot(history3.history['val_loss'])\n",
    "      axes[1].plot(history4.history['val_loss'])\n",
    "      axes[1].plot(history5.history['val_loss'])\n",
    "      axes[1].legend(['0.9', '0.8', '0.7', '0.6', '0.5'], loc='best')\n",
    "    except:\n",
    "      pass\n",
    "    axes[1].set_title('Validation Loss Vs. No of Epochs for Various Momentum Values')\n",
    "    axes[1].set_ylabel('Validation Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "  else:\n",
    "    plt.plot(history.history['loss'])\n",
    "    try:\n",
    "      plt.plot(history.history['val_loss'])\n",
    "      plt.legend(['Train', 'Val'])\n",
    "    except:\n",
    "      pass\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "  \n",
    "plot_val_history(denoise_history_sgd_mo1, denoise_history_sgd_mo2, denoise_history_sgd_mo3, denoise_history_sgd_mo4, denoise_history_sgd_mo5, 'mean_absolute_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgd = keras.optimizers.SGD(lr=0.1, momentum=0.9, nesterov=True)\n",
    "\n",
    "denoise_model_base.compile(loss='mean_absolute_error', optimizer=sgd, metrics=['mae'])\n",
    "\n",
    "\n",
    "\n",
    "#epochs = 1\n",
    "### Use a loop to save for each epoch the weights in an external website in\n",
    "### case colab stops. Every time you call fit/fit_generator the weigths are NOT\n",
    "### reset, so e.g. calling 5 times fit(epochs=1) behave as fit(epochs=5)\n",
    "#for e in range(epochs):\n",
    "denoise_history_base = denoise_model_base.fit_generator(generator=denoise_generator, \n",
    "                                            epochs=20, verbose=1, \n",
    "                                            validation_data=denoise_generator_val)\n",
    "\n",
    "### Saves optimizer and weights\n",
    "denoise_model_base.save('denoise_base.h5')\n",
    "\n",
    "### Uploads files to external hosting\n",
    "#!curl -F \"file=@denoise.h5\" https://file.io"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "improvedupon_code.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/MatchLab-Imperial/keras_triplet_descriptor/blob/master/Baseline_code.ipynb",
     "timestamp": 1552754449861
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
