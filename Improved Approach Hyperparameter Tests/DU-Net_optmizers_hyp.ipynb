{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12807,
     "status": "ok",
     "timestamp": 1552773429425,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "ZZG4BqkENEyd",
    "outputId": "0b8bd808-eba5-4de2-9c01-64058b721fba"
   },
   "outputs": [],
   "source": [
    "# Taken from\n",
    "# https://stackoverflow.com/questions/48750199/google-colaboratory-misleading-information-about-its-gpu-only-5-ram-available\n",
    "# memory footprint support libraries/code\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "# Colab only provides one GPU and it is not always guaranteed\n",
    "gpu = GPUs[0]\n",
    "def printm():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(\"RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2475,
     "status": "ok",
     "timestamp": 1552773429426,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "BBvIvBoyg68g",
    "outputId": "64e90310-ddee-4cb4-a672-41a831cc0503"
   },
   "outputs": [],
   "source": [
    "printm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4124,
     "status": "ok",
     "timestamp": 1552773455053,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "yV1m-9ZGuKGj",
    "outputId": "32fde251-f5a5-4b68-9b12-9e42974bffae"
   },
   "outputs": [],
   "source": [
    "# Clone repo\n",
    "!git clone https://github.com/MatchLab-Imperial/keras_triplet_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 978,
     "status": "ok",
     "timestamp": 1552773459405,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "pyZSqhZ5LACT",
    "outputId": "ce0a5f2e-73e9-43f0-f80b-41950680731a"
   },
   "outputs": [],
   "source": [
    "# Change directory\n",
    "%cd keras_triplet_descriptor    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 190209,
     "status": "ok",
     "timestamp": 1552773652267,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "307CBCL-FjX4",
    "outputId": "9b10a27f-fe9c-4492-a089-784596f08ac0"
   },
   "outputs": [],
   "source": [
    "# Download data\n",
    "!wget -O hpatches_data.zip https://imperialcollegelondon.box.com/shared/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 249292,
     "status": "ok",
     "timestamp": 1552773724185,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "36mBTFvPCxY9",
    "outputId": "b0b44eeb-fe9c-4c11-e2e1-8a67725abd68"
   },
   "outputs": [],
   "source": [
    "# Extract data\n",
    "!unzip -q ./hpatches_data.zip\n",
    "!rm ./hpatches_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0KYfe-at9KN"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda, Reshape\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Conv2DTranspose\n",
    "from keras.layers import Input, UpSampling2D, concatenate  \n",
    "\n",
    "from read_data import HPatches, DataGeneratorDesc, hpatches_sequence_folder, DenoiseHPatches, tps\n",
    "from utils import generate_desc_csv, plot_denoise, plot_triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NXL31ez-AT5h"
   },
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ABKDHB9RApZk"
   },
   "outputs": [],
   "source": [
    "hpatches_dir = './hpatches'\n",
    "splits_path = 'splits.json'\n",
    "\n",
    "splits_json = json.load(open(splits_path, 'rb'))\n",
    "split = splits_json['a']\n",
    "\n",
    "train_fnames = split['train']\n",
    "test_fnames = split['test']\n",
    "\n",
    "seqs = glob.glob(hpatches_dir+'/*')\n",
    "seqs = [os.path.abspath(p) for p in seqs]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs_train = list(filter(lambda x: x.split('\\\\')[-1] in train_fnames, seqs)) \n",
    "seqs_test = list(filter(lambda x: x.split('\\\\')[-1] in split['test'], seqs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(seqs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(seqs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qeWik0vMEtuC"
   },
   "source": [
    "## Models and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6QbkHnbuIUD"
   },
   "outputs": [],
   "source": [
    "def get_denoise_model(shape, do=0):\n",
    "  \n",
    "    inputs = Input(shape)\n",
    "    \n",
    "    conv1 = Dropout(do)((Conv2D(32, (3, 3), activation = 'selu', padding='same')(inputs)))\n",
    "    conv1 = Dropout(do)((Conv2D(32, (3, 3), activation = 'selu', padding='same')(conv1)))\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Dropout(do)((Conv2D(64, (3, 3), activation = 'selu', padding='same')(pool1)))\n",
    "    conv2 = Dropout(do)((Conv2D(64, (3, 3), activation = 'selu', padding='same')(conv2)))\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Dropout(do)((Conv2D(128, (3, 3), activation = 'selu', padding='same')(pool2)))\n",
    "    conv3 = Dropout(do)((Conv2D(128, (3, 3), activation = 'selu', padding='same')(conv3)))\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Dropout(do)((Conv2D(256, (3, 3), activation = 'selu', padding='same')(pool3)))\n",
    "    conv4 = Dropout(do)((Conv2D(256, (3, 3), activation = 'selu', padding='same')(conv4)))\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Dropout(do)((Conv2D(512, (3, 3), activation = 'selu', padding='same')(pool4)))\n",
    "    conv5 = Dropout(do)((Conv2D(512, (3, 3), activation = 'selu', padding='same')(conv5)))\n",
    "\n",
    "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
    "    conv6 = Dropout(do)((Conv2D(256, (3, 3), activation = 'selu', padding='same')(up6)))\n",
    "    conv6 = Dropout(do)((Conv2D(256, (3, 3), activation = 'selu', padding='same')(conv6)))\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
    "    conv7 = Dropout(do)((Conv2D(128, (3, 3), activation = 'selu', padding='same')(up7)))\n",
    "    conv7 = Dropout(do)((Conv2D(128, (3, 3), activation = 'selu', padding='same')(conv7)))\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
    "    conv8 = Dropout(do)((Conv2D(64, (3, 3), activation = 'selu', padding='same')(up8)))\n",
    "    conv8 = Dropout(do)((Conv2D(64, (3, 3), activation = 'selu', padding='same')(conv8)))\n",
    "\n",
    "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
    "    conv9 = Dropout(do)((Conv2D(32, (3, 3), activation = 'selu', padding='same')(up9)))\n",
    "    conv9 = Dropout(do)((Conv2D(32, (3, 3), activation = 'selu', padding='same')(conv9)))\n",
    "\n",
    "    conv10 = Dropout(do)(Conv2D(1, (1, 1))(conv9))\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[conv10])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_descriptor_model(shape):\n",
    "  \n",
    "    '''Architecture copies HardNet architecture'''\n",
    "\n",
    "    init_weights = keras.initializers.he_normal()\n",
    "\n",
    "    descriptor_model = Sequential()\n",
    "    descriptor_model.add(Conv2D(32, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights))\n",
    "    descriptor_model.add(BatchNormalization(axis = -1))\n",
    "    descriptor_model.add(Activation('relu'))\n",
    "\n",
    "    descriptor_model.add(Conv2D(32, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
    "    descriptor_model.add(BatchNormalization(axis = -1))\n",
    "    descriptor_model.add(Activation('relu'))\n",
    "\n",
    "    descriptor_model.add(Conv2D(64, 3, padding='same', strides=2, use_bias = True, kernel_initializer=init_weights))\n",
    "    descriptor_model.add(BatchNormalization(axis = -1))\n",
    "    descriptor_model.add(Activation('relu'))\n",
    "\n",
    "    descriptor_model.add(Conv2D(64, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
    "    descriptor_model.add(BatchNormalization(axis = -1))\n",
    "    descriptor_model.add(Activation('relu'))\n",
    "\n",
    "    descriptor_model.add(Conv2D(128, 3, padding='same', strides=2,  use_bias = True, kernel_initializer=init_weights))\n",
    "    descriptor_model.add(BatchNormalization(axis = -1))\n",
    "    descriptor_model.add(Activation('relu'))\n",
    "\n",
    "    descriptor_model.add(Conv2D(128, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
    "    descriptor_model.add(BatchNormalization(axis = -1))\n",
    "    descriptor_model.add(Activation('relu'))\n",
    "    descriptor_model.add(Dropout(0.3))\n",
    "\n",
    "    descriptor_model.add(Conv2D(128, 8, padding='valid', use_bias = True, kernel_initializer=init_weights))\n",
    "\n",
    "    # Final descriptor reshape\n",
    "    descriptor_model.add(Reshape((128,)))\n",
    "\n",
    "    return descriptor_model\n",
    "  \n",
    "\n",
    "def triplet_loss(x):\n",
    "  \n",
    "    output_dim = 128\n",
    "    a, p, n = x\n",
    "    _alpha = 1.0\n",
    "    positive_distance = K.mean(K.square(a - p), axis=-1)\n",
    "    negative_distance = K.mean(K.square(a - n), axis=-1)\n",
    "\n",
    "    return K.expand_dims(K.maximum(0.0, positive_distance - negative_distance + _alpha), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RlS5zcV7EJgp"
   },
   "source": [
    "## Denoising Image Patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 113505,
     "status": "ok",
     "timestamp": 1552773869491,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "m_VPSHmSK0dS",
    "outputId": "50e9d1d1-368e-4ae8-ba54-d3dba09d503b"
   },
   "outputs": [],
   "source": [
    "#denoise_generator = DenoiseHPatches(random.sample(seqs_train, 3), batch_size=50)\n",
    "#denoise_generator_val = DenoiseHPatches(random.sample(seqs_test, 1), batch_size=50)\n",
    "\n",
    "# Uncomment following lines for using all the data to train the denoising model\n",
    "denoise_generator = DenoiseHPatches(seqs_train, batch_size=50)\n",
    "denoise_generator_val = DenoiseHPatches(seqs_test, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1293,
     "status": "ok",
     "timestamp": 1552773922597,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "-eUSba93Dttj",
    "outputId": "9888dc14-c015-469b-f13c-d1576f667cbb"
   },
   "outputs": [],
   "source": [
    "shape = (32, 32, 1)\n",
    "denoise_model_adam = get_denoise_model(shape)\n",
    "denoise_model_adam_amsgrad = get_denoise_model(shape)\n",
    "denoise_model_sgd = get_denoise_model(shape)\n",
    "denoise_model_adadelta = get_denoise_model(shape)\n",
    "denoise_model_nadam = get_denoise_model(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1417,
     "status": "error",
     "timestamp": 1552811688250,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "edwbgE6yKqcD",
    "outputId": "d633c862-209f-4455-85be-4a19155c50f9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "adam_amsgrad = keras.optimizers.Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "sgd = keras.optimizers.SGD(lr=0.00001, momentum=0.9, nesterov=True)\n",
    "adadelta =  keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "nadam = keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "\n",
    "\n",
    "denoise_model_adam.compile(loss='mean_absolute_error', optimizer=adam, metrics=['mae'])\n",
    "denoise_model_adam_amsgrad.compile(loss='mean_absolute_error', optimizer=adam_amsgrad, metrics=['mae'])\n",
    "denoise_model_sgd.compile(loss='mean_absolute_error', optimizer=sgd, metrics=['mae'])\n",
    "denoise_model_adadelta.compile(loss='mean_absolute_error', optimizer=adadelta, metrics=['mae'])\n",
    "denoise_model_nadam.compile(loss='mean_absolute_error', optimizer=nadam, metrics=['mae'])\n",
    "\n",
    "#epochs = 1\n",
    "### Use a loop to save for each epoch the weights in an external website in\n",
    "### case colab stops. Every time you call fit/fit_generator the weigths are NOT\n",
    "### reset, so e.g. calling 5 times fit(epochs=1) behave as fit(epochs=5)\n",
    "#for e in range(epochs):\n",
    "denoise_history_adam = denoise_model_adam.fit_generator(generator=denoise_generator, \n",
    "                                            epochs=5, verbose=1, \n",
    "                                            validation_data=denoise_generator_val)\n",
    "\n",
    "denoise_history_adam_amsgrad = denoise_model_adam_amsgrad.fit_generator(generator=denoise_generator, \n",
    "                                            epochs=5, verbose=1, \n",
    "                                            validation_data=denoise_generator_val)\n",
    "\n",
    "denoise_history_sgd = denoise_model_sgd.fit_generator(generator=denoise_generator, \n",
    "                                            epochs=5, verbose=1, \n",
    "                                            validation_data=denoise_generator_val)\n",
    "\n",
    "denoise_history_adadelta = denoise_model_adadelta.fit_generator(generator=denoise_generator, \n",
    "                                            epochs=5, verbose=1, \n",
    "                                            validation_data=denoise_generator_val)\n",
    "\n",
    "denoise_history_nadam = denoise_model_nadam.fit_generator(generator=denoise_generator, \n",
    "                                            epochs=5, verbose=1, \n",
    "                                            validation_data=denoise_generator_val)\n",
    "### Saves optimizer and weights\n",
    "#denoise_model_adam.save('denoise_adam.h5')\n",
    "#denoise_model_adam_amsgrad.save('denoise_adam_amsgrad.h5')\n",
    "#denoise_model_sgd.save('denoise_sgd.h5')\n",
    "#denoise_model_adadelta.save('denoise_sgd.h5')\n",
    "#denoise_model_nadam.save('denoise_sgd.h5')\n",
    "\n",
    "\n",
    "### Uploads files to external hosting\n",
    "#!curl -F \"file=@denoise.h5\" https://file.io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del denoise_generator\n",
    "del denoise_generator_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e9FzSZzMEcs4"
   },
   "source": [
    "### Visualization of Denoising Results\n",
    "To visualize how the denoised patches look, you can run the following function. It returns the noisy patch, the denoised patch in the middle, and the clean patch in the right side. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XFA_8uN4Eb3B"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_history(history, history2, history3, history4, history5, metric = None):\n",
    "  # Plots the loss history of training and validation (if existing)\n",
    "  # and a given metric\n",
    "  \n",
    "  if metric != None:\n",
    "    fig, axes = plt.subplots(2,1, figsize=(8, 10))\n",
    "    axes[0].plot(history.history[metric])\n",
    "    axes[0].plot(history2.history[metric])\n",
    "    axes[0].plot(history3.history[metric])\n",
    "    axes[0].plot(history4.history[metric])\n",
    "    axes[0].plot(history5.history[metric])\n",
    "    \n",
    "    try:\n",
    "      #axes[0].plot(history.history['val_'+metric])\n",
    "      #axes[0].plot(history2.history['val2_'+metric])\n",
    "      #axes[0].plot(history3.history['val3_'+metric])\n",
    "      axes[0].legend(['Adam', 'Adam_Amsgrad', 'SGD', 'Adadelta', 'Nadam' ], loc='best')\n",
    "    except:\n",
    "      pass\n",
    "    axes[0].set_title('MAE Vs. No of Epochs for Various Optimzers')\n",
    "    axes[0].set_ylabel('Mean Absolute Error')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    axes[1].plot(history.history['loss'])\n",
    "    axes[1].plot(history2.history['loss'])\n",
    "    axes[1].plot(history3.history['loss'])\n",
    "    axes[1].plot(history4.history['loss'])\n",
    "    axes[1].plot(history5.history['loss'])\n",
    "    try:\n",
    "      #axes[1].plot(history.history['val_loss'])\n",
    "      axes[1].legend(['Adam', 'Adam_Amsgrad', 'SGD', 'Adadelta', 'Nadam' ])\n",
    "    except:\n",
    "      pass\n",
    "    axes[1].set_title('MAE Vs. No of Epochs for Various Optimzers')\n",
    "    axes[1].set_ylabel('Mean Absolute Error')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "  else:\n",
    "    plt.plot(history.history['loss'])\n",
    "    try:\n",
    "      plt.plot(history.history['val_loss'])\n",
    "      plt.legend(['Train', 'Val'])\n",
    "    except:\n",
    "      pass\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "  \n",
    "plot_history(denoise_history_adam, denoise_history_adam_amsgrad, denoise_history_sgd, denoise_history_adadelta, denoise_history_nadam, 'mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_val_history(history, history2, history3, history4, history5, metric = None):\n",
    "  # Plots the loss history of training and validation (if existing)\n",
    "  # and a given metric\n",
    "  \n",
    "  if metric != None:\n",
    "    fig, axes = plt.subplots(2,1, figsize=(8, 10))\n",
    "    #axes[0].plot(history.history[metric])\n",
    "    #axes[0].plot(history2.history[metric])\n",
    "    #axes[0].plot(history3.history[metric])\n",
    "    try:\n",
    "      axes[0].plot(history.history['val_'+metric])\n",
    "      axes[0].plot(history2.history['val_'+metric])\n",
    "      axes[0].plot(history3.history['val_'+metric])\n",
    "      axes[0].plot(history4.history['val_'+metric])\n",
    "      axes[0].plot(history5.history['val_'+metric])\n",
    "      axes[0].legend(['Adam', 'Adam_Amsgrad', 'SGD', 'Adadelta', 'Nadam'])\n",
    "    except:\n",
    "      pass\n",
    "    axes[0].set_title('Validation Loss Vs. No of Epochs for for Various Optimzers')\n",
    "    axes[0].set_ylabel('Validation Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    #axes[1].plot(history.history['loss'])\n",
    "    #axes[1].plot(history2.history['loss'])\n",
    "    #axes[1].plot(history3.history['loss'])\n",
    "    try:\n",
    "      axes[1].plot(history.history['val_loss'])\n",
    "      axes[1].plot(history2.history['val_loss'])\n",
    "      axes[1].plot(history3.history['val_loss'])\n",
    "      axes[1].plot(history4.history['val_loss'])\n",
    "      axes[1].plot(history5.history['val_loss'])\n",
    "      axes[1].legend(['Adam', 'Adam_Amsgrad', 'SGD', 'Adadelta', 'Nadam'])\n",
    "    except:\n",
    "      pass\n",
    "    axes[1].set_title('Validation Loss Vs. No of Epochs for Various Optimzers')\n",
    "    axes[1].set_ylabel('Validation Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "  else:\n",
    "    plt.plot(history.history['loss'])\n",
    "    try:\n",
    "      plt.plot(history.history['val_loss'])\n",
    "      plt.legend(['Train', 'Val'])\n",
    "    except:\n",
    "      pass\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "  \n",
    "plot_val_history(denoise_history_adam, denoise_history_adam_amsgrad, denoise_history_sgd, denoise_history_adadelta, denoise_history_nadam, 'mean_absolute_error')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "improvedupon_code.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/MatchLab-Imperial/keras_triplet_descriptor/blob/master/Baseline_code.ipynb",
     "timestamp": 1552754449861
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
