{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12807,
     "status": "ok",
     "timestamp": 1552773429425,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "ZZG4BqkENEyd",
    "outputId": "0b8bd808-eba5-4de2-9c01-64058b721fba"
   },
   "outputs": [],
   "source": [
    "# Taken from\n",
    "# https://stackoverflow.com/questions/48750199/google-colaboratory-misleading-information-about-its-gpu-only-5-ram-available\n",
    "# memory footprint support libraries/code\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "# Colab only provides one GPU and it is not always guaranteed\n",
    "gpu = GPUs[0]\n",
    "def printm():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(\"RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2475,
     "status": "ok",
     "timestamp": 1552773429426,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "BBvIvBoyg68g",
    "outputId": "64e90310-ddee-4cb4-a672-41a831cc0503"
   },
   "outputs": [],
   "source": [
    "printm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4124,
     "status": "ok",
     "timestamp": 1552773455053,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "yV1m-9ZGuKGj",
    "outputId": "32fde251-f5a5-4b68-9b12-9e42974bffae"
   },
   "outputs": [],
   "source": [
    "# Clone repo\n",
    "!git clone https://github.com/MatchLab-Imperial/keras_triplet_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 978,
     "status": "ok",
     "timestamp": 1552773459405,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "pyZSqhZ5LACT",
    "outputId": "ce0a5f2e-73e9-43f0-f80b-41950680731a"
   },
   "outputs": [],
   "source": [
    "# Change directory\n",
    "%cd keras_triplet_descriptor    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 190209,
     "status": "ok",
     "timestamp": 1552773652267,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "307CBCL-FjX4",
    "outputId": "9b10a27f-fe9c-4492-a089-784596f08ac0"
   },
   "outputs": [],
   "source": [
    "# Download data\n",
    "!wget -O hpatches_data.zip https://imperialcollegelondon.box.com/shared/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 249292,
     "status": "ok",
     "timestamp": 1552773724185,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "36mBTFvPCxY9",
    "outputId": "b0b44eeb-fe9c-4c11-e2e1-8a67725abd68"
   },
   "outputs": [],
   "source": [
    "# Extract data\n",
    "!unzip -q ./hpatches_data.zip\n",
    "!rm ./hpatches_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rjyr96hR_4wS"
   },
   "source": [
    "## Importing Necessary Modules\n",
    "\n",
    "We now import the modules we will use in this baseline code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0KYfe-at9KN"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda, Reshape\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Conv2DTranspose\n",
    "from keras.layers import Input, UpSampling2D, concatenate  \n",
    "\n",
    "from read_data import HPatches, DataGeneratorDesc, hpatches_sequence_folder, DenoiseHPatches, tps\n",
    "from utils import generate_desc_csv, plot_denoise, plot_triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NXL31ez-AT5h"
   },
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ABKDHB9RApZk"
   },
   "outputs": [],
   "source": [
    "hpatches_dir = './hpatches'\n",
    "splits_path = 'splits.json'\n",
    "\n",
    "splits_json = json.load(open(splits_path, 'rb'))\n",
    "split = splits_json['a']\n",
    "\n",
    "train_fnames = split['train']\n",
    "test_fnames = split['test']\n",
    "\n",
    "seqs = glob.glob(hpatches_dir+'/*')\n",
    "seqs = [os.path.abspath(p) for p in seqs]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs_train = list(filter(lambda x: x.split('\\\\')[-1] in train_fnames, seqs)) \n",
    "seqs_test = list(filter(lambda x: x.split('\\\\')[-1] in split['test'], seqs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(seqs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(seqs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qeWik0vMEtuC"
   },
   "source": [
    "## Models and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6QbkHnbuIUD"
   },
   "outputs": [],
   "source": [
    "def get_denoise_model(shape, do = 0, activate = 'selu'):\n",
    "  \n",
    "    inputs = Input(shape)\n",
    "    \n",
    "    conv1 = Dropout(do)((Conv2D(32, (3, 3), activation = activate, padding='same')(inputs)))\n",
    "    conv1 = Dropout(do)((Conv2D(32, (3, 3), activation = activate, padding='same')(conv1)))\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Dropout(do)((Conv2D(64, (3, 3), activation =  activate, padding='same')(pool1)))\n",
    "    conv2 = Dropout(do)((Conv2D(64, (3, 3), activation =  activate, padding='same')(conv2)))\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Dropout(do)((Conv2D(128, (3, 3), activation =  activate, padding='same')(pool2)))\n",
    "    conv3 = Dropout(do)((Conv2D(128, (3, 3), activation =  activate, padding='same')(conv3)))\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Dropout(do)((Conv2D(256, (3, 3), activation =  activate, padding='same')(pool3)))\n",
    "    conv4 = Dropout(do)((Conv2D(256, (3, 3), activation =  activate , padding='same')(conv4)))\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Dropout(do)((Conv2D(512, (3, 3), activation =  activate, padding='same')(pool4)))\n",
    "    conv5 = (Conv2D(512, (3, 3), activation =  activate, padding='same'))(UpSampling2D(size = (2,2))(conv5))\n",
    "\n",
    "    up6 = concatenate([conv5, conv4], axis=3)\n",
    "    conv6 = Dropout(do)((Conv2D(256, (3, 3), activation =  activate, padding='same')(up6)))\n",
    "    conv6 = (Conv2D(256, (3, 3), activation =  activate, padding='same'))(UpSampling2D(size = (2,2))(conv6))\n",
    "\n",
    "    up7 = concatenate([conv6, conv3], axis=3)\n",
    "    conv7 = Dropout(do)((Conv2D(128, (3, 3), activation =  activate, padding='same')(up7)))\n",
    "    conv7 = (Conv2D(128, (3, 3), activation =  activate, padding='same'))(UpSampling2D(size = (2,2))(conv7))\n",
    "\n",
    "    up8 = concatenate([conv7, conv2], axis=3)\n",
    "    conv8 = Dropout(do)((Conv2D(64, (3, 3), activation =  activate, padding='same')(up8)))\n",
    "    conv8 = (Conv2D(64, (3, 3), activation =  activate, padding='same'))(UpSampling2D(size = (2,2))(conv8))\n",
    "\n",
    "    up9 = concatenate([conv8, conv1], axis=3)\n",
    "    conv9 = Dropout(do)((Conv2D(32, (3, 3), activation =  activate, padding='same')(up9)))\n",
    "    conv9 = Dropout(do)((Conv2D(32, (3, 3), activation =  activate, padding='same')(conv9)))\n",
    "\n",
    "    conv10 = Dropout(do)(Conv2D(1, (1, 1))(conv9))\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[conv10])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def get_descriptor_model(shape, activate= 'elu'):\n",
    "  \n",
    "    '''Architecture copies HardNet architecture'''\n",
    "\n",
    "    init_weights = keras.initializers.he_normal()\n",
    "\n",
    "    descriptor_model = Sequential()\n",
    "    descriptor_model.add(Conv2D(32, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights))\n",
    "    descriptor_model.add(BatchNormalization(axis = -1))\n",
    "    descriptor_model.add(Activation(activate))\n",
    "\n",
    "    descriptor_model.add(Conv2D(32, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
    "    descriptor_model.add(BatchNormalization(axis = -1))\n",
    "    descriptor_model.add(Activation(activate))\n",
    "\n",
    "    descriptor_model.add(Conv2D(64, 3, padding='same', strides=2, use_bias = True, kernel_initializer=init_weights))\n",
    "    descriptor_model.add(BatchNormalization(axis = -1))\n",
    "    descriptor_model.add(Activation(activate))\n",
    "\n",
    "    descriptor_model.add(Conv2D(64, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
    "    descriptor_model.add(BatchNormalization(axis = -1))\n",
    "    descriptor_model.add(Activation(activate))\n",
    "\n",
    "    descriptor_model.add(Conv2D(128, 3, padding='same', strides=2,  use_bias = True, kernel_initializer=init_weights))\n",
    "    descriptor_model.add(BatchNormalization(axis = -1))\n",
    "    descriptor_model.add(Activation(activate))\n",
    "\n",
    "    descriptor_model.add(Conv2D(128, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
    "    descriptor_model.add(BatchNormalization(axis = -1))\n",
    "    descriptor_model.add(Activation(activate))\n",
    "    descriptor_model.add(Dropout(0.5))\n",
    "\n",
    "    descriptor_model.add(Conv2D(128, 8, padding='valid', use_bias = True, kernel_initializer=init_weights))\n",
    "\n",
    "    # Final descriptor reshape\n",
    "    descriptor_model.add(Reshape((128,)))\n",
    "\n",
    "    return descriptor_model\n",
    "  \n",
    "\n",
    "def triplet_loss(x):\n",
    "  \n",
    "    output_dim = 128\n",
    "    a, p, n = x\n",
    "    _alpha = 1.0\n",
    "    positive_distance = K.mean(K.square(a - p), axis=-1)\n",
    "    negative_distance = K.mean(K.square(a - n), axis=-1)\n",
    "\n",
    "    return K.expand_dims(K.maximum(0.0, positive_distance - negative_distance + _alpha), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RlS5zcV7EJgp"
   },
   "source": [
    "## Denoising Image Patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1293,
     "status": "ok",
     "timestamp": 1552773922597,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "-eUSba93Dttj",
    "outputId": "9888dc14-c015-469b-f13c-d1576f667cbb"
   },
   "outputs": [],
   "source": [
    "from keras.layers import LeakyReLU\n",
    "shape = (32, 32, 1)\n",
    "denoise_model = keras.models.load_model('./denoise_modified.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SyABaCvkEPDR"
   },
   "source": [
    "## Training a Descriptor Network\n",
    "In the last section we trained a model that given a noisy patch, outputs a denoised version of it. We hoped that by doing so, we will improve the performance of the second part, which is training a network that outputs the descriptor. As we mentioned, a descriptor is a numerical vector that represents the small images we have. The dataset consists of a large number of small images, which are cropped patches from other larger images. Hence, they represent some local part of a scene. That is why there are no objects represented, only corners or textures. Each of these patches is related to a subset of other patches of the dataset by some kind of geometric transformation (e.g. rotation).  For a given patch, we want the network to output a vector that is close to the vectors of the patches that represent the same local part of a scene, while being far from patches do not represent that local part of a scene.\n",
    "\n",
    "To do so, we will build a convolutional neural network that takes the input of $32\\times32$ and outputs a descriptor of size $128$. For the loss, we use the triplet loss, which takes an anchor patch, a negative patch and a positive patch. The idea is to train the network so the descriptors from the anchor and positive patch have a low distance between them, and the negative and anchor patch have a large distance between them. \n",
    "\n",
    "In this cell we generate a triplet network, which is a network formed by three copies of the same network. That means that the descriptor model will compute the descriptor for the input `'a'` (anchor), the same descriptor model (with the same weights) will compute the descriptor for the input `'p'` (positive), and again the same model will compute the descriptor for the input `'n'` (negative). \n",
    "\n",
    "**Updated explanation**: Due to the way Keras handles the compile method, it needs a loss as an argument in that compile method. However, our loss is computed in the lambda layer, so we want to minimize the output of that layer. As we want to minimize the output of the Lambda function (in this case the triplet loss), we output as the label in the training_generator a vector of zeros and we compute the mean absolute error of the triplet loss and this vector of zeros. To give you an intuition, what we aim to minimize is\n",
    "$$  |\\text{triplet_loss} - 0| =  |\\text{triplet_loss}| = \\text{triplet_loss} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "DVmDZIRTHPDa",
    "outputId": "94bda2ad-e40e-4c44-e74f-be30d956eed6"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Lambda\n",
    "shape = (32, 32, 1)\n",
    "xa = Input(shape=shape, name='a')\n",
    "xp = Input(shape=shape, name='p')\n",
    "xn = Input(shape=shape, name='n')\n",
    "\n",
    "descriptor_model_relu = get_descriptor_model( shape)\n",
    "ea = descriptor_model(xa)\n",
    "ep = descriptor_model(xp)\n",
    "en = descriptor_model(xn)\n",
    "loss = Lambda(triplet_loss)([ea, ep, en])\n",
    "\n",
    "\n",
    "adam_amsgrad = keras.optimizers.Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "sgd = keras.optimizers.SGD(lr=0.00001, momentum=0.9, nesterov=True)\n",
    "adadelta =  keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "nadam = keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "descriptor_model_trip_adam_amsgrad = Model(inputs=[xa, xp, xn], outputs=loss)\n",
    "descriptor_model_trip_sgd = Model(inputs=[xa, xp, xn], outputs=loss)\n",
    "descriptor_model_trip_adadelta = Model(inputs=[xa, xp, xn], outputs=loss)\n",
    "descriptor_model_trip_nadam = Model(inputs=[xa, xp, xn], outputs=loss)\n",
    "descriptor_model_trip_adam = Model(inputs=[xa, xp, xn], outputs=loss)\n",
    "\n",
    "\n",
    "descriptor_model_trip_adam_amsgrad.compile(loss='mean_absolute_error', optimizer=adam)\n",
    "descriptor_model_trip_sgd.compile(loss='mean_absolute_error', optimizer=adam_amsgrad)\n",
    "descriptor_model_trip_adadelta.compile(loss='mean_absolute_error', optimizer=adadelta)\n",
    "descriptor_model_trip_nadam.compile(loss='mean_absolute_error', optimizer=sgd)\n",
    "descriptor_model_trip_adam.compile(loss='mean_absolute_error', optimizer=nadam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "YIR1cH4fDwKj",
    "outputId": "b17c0b36-e90e-4df0-e969-0ab13136786d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Descriptor loading and training\n",
    "# Loading images\n",
    "hPatches = HPatches(train_fnames=train_fnames, test_fnames=test_fnames,\n",
    "                    denoise_model=denoise_model, use_clean=False)\n",
    "\n",
    "# Creating training generator\n",
    "training_generator = DataGeneratorDesc(*hPatches.read_image_file(hpatches_dir, train=1), num_triplets=100000)\n",
    "# Creating validation generator\n",
    "val_generator = DataGeneratorDesc(*hPatches.read_image_file(hpatches_dir, train=0), num_triplets=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "colab_type": "code",
    "id": "3RQmOMU92csu",
    "outputId": "648c64db-a797-4bb6-c7de-5eb45d27da25"
   },
   "outputs": [],
   "source": [
    "plot_triplet(training_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "QPyc8as42WTQ",
    "outputId": "69ea09ee-8198-42d5-a654-2bd3e339e5b2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#epochs = 1\n",
    "### As with the denoising model, we use a loop to save for each epoch \n",
    "## #the weights in an external website in case colab stops. \n",
    "### reset, so e.g. calling 5 times fit(epochs=1) behave as fit(epochs=5)\n",
    "\n",
    "### If you have a model saved from a previous training session\n",
    "### Load it in the next line\n",
    "# descriptor_model_trip.set_weights(keras.models.load_model('./descriptor.h5').get_weights())\n",
    "# descriptor_model_trip.optimizer = keras.models.load_model('./descriptor.h5').optimizer\n",
    "\n",
    "#for e in range(epochs):\n",
    "  \n",
    "descriptor_history_adam_amsgrad = descriptor_model_trip_adam_amsgrad.fit_generator(generator=training_generator, epochs=5, verbose=1, validation_data=val_generator)\n",
    "\n",
    "descriptor_history_sgd = descriptor_model_trip_sgd.fit_generator(generator=training_generator, epochs=5, verbose=1, validation_data=val_generator)\n",
    "\n",
    "descriptor_history_adadelta = descriptor_model_trip_adadelta.fit_generator(generator=training_generator, epochs=5, verbose=1, validation_data=val_generator)\n",
    "\n",
    "descriptor_history_nadam = descriptor_model_trip_nadam.fit_generator(generator=training_generator, epochs=5, verbose=1, validation_data=val_generator)\n",
    "\n",
    "descriptor_history_adam = descriptor_model_trip_adam.fit_generator(generator=training_generator, epochs=5, verbose=1, validation_data=val_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XFA_8uN4Eb3B"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, history2, history3, history4, history5, metric = None):\n",
    "  # Plots the loss history of training and validation (if existing)\n",
    "  # and a given metric\n",
    "  \n",
    "  if metric != None:\n",
    "    fig, axes = plt.subplots(2,1, figsize=(8, 10))\n",
    "    #axes[0].plot(history.history[metric])\n",
    "    #axes[0].plot(history2.history[metric])\n",
    "    #axes[0].plot(history3.history[metric])\n",
    "    #axes[0].plot(history4.history[metric])\n",
    "    #axes[0].plot(history5.history[metric])\n",
    "    #axes[0].plot(history6.history[metric])\n",
    "    try:\n",
    "      #axes[0].plot(history.history['val_'+metric])\n",
    "      #axes[0].plot(history2.history['val2_'+metric])\n",
    "      #axes[0].plot(history3.history['val3_'+metric])\n",
    "      #axes[0].legend(['ReLU', 'ELU', 'SELU', 'Linear', 'LeakyReLU'], loc='upper right')\n",
    "    except:\n",
    "      pass\n",
    "    #axes[0].set_title('MAE Vs. No of Epochs for Various Activation Functions')\n",
    "    #axes[0].set_ylabel('Mean Absolute Error')\n",
    "    #axes[0].set_xlabel('Epoch')\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    axes[1].plot(history.history['loss'])\n",
    "    axes[1].plot(history2.history['loss'])\n",
    "    axes[1].plot(history3.history['loss'])\n",
    "    axes[1].plot(history4.history['loss'])\n",
    "    axes[1].plot(history5.history['loss'])\n",
    "    try:\n",
    "      #axes[1].plot(history.history['val_loss'])\n",
    "      axes[1].legend(['Adam', 'Adam_Amsgrad', 'Adadelta', 'SGD', 'Nadam'], loc='upper right')\n",
    "    except:\n",
    "      pass\n",
    "    axes[1].set_title('MAE Vs. No of Epochs for Various Optimizers')\n",
    "    axes[1].set_ylabel('Mean Absolute Error')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "  else:\n",
    "    plt.plot(history.history['loss'])\n",
    "    try:\n",
    "      plt.plot(history.history['val_loss'])\n",
    "      plt.legend(['Train', 'Val'])\n",
    "    except:\n",
    "      pass\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "  \n",
    "plot_history(descriptor_history_adam, descriptor_history_adam_amsgrad, descriptor_history_adadelta, descriptor_history_sgd, descriptor_history_nadam, 'mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_val_history(history, history2, history3, history4, history5, metric = None):\n",
    "  # Plots the loss history of training and validation (if existing)\n",
    "  # and a given metric\n",
    "  \n",
    "  if metric != None:\n",
    "    fig, axes = plt.subplots(2,1, figsize=(8, 10))\n",
    "    #axes[0].plot(history.history[metric])\n",
    "    #axes[0].plot(history2.history[metric])\n",
    "    #axes[0].plot(history3.history[metric])\n",
    "    try:\n",
    "      #axes[0].plot(history.history['val_'+metric])\n",
    "      #axes[0].plot(history2.history['val_'+metric])\n",
    "      #axes[0].plot(history3.history['val_'+metric])\n",
    "      #axes[0].plot(history4.history['val_'+metric])\n",
    "      #axes[0].plot(history5.history['val_'+metric])\n",
    "      #axes[0].plot(history6.history['val_'+metric])\n",
    "      #axes[0].legend(['ReLU', 'ELU', 'SELU', 'Linear', 'LeakyReLU'], loc='upper right')\n",
    "\n",
    "    except:\n",
    "      pass\n",
    "    #axes[0].set_title('Validation Loss Vs. No of Epochs for Various Activation Functions')\n",
    "    #axes[0].set_ylabel('Validation Loss')\n",
    "    #axes[0].set_xlabel('Epoch')\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    #axes[1].plot(history.history['loss'])\n",
    "    #axes[1].plot(history2.history['loss'])\n",
    "    #axes[1].plot(history3.history['loss'])\n",
    "    try:\n",
    "      axes[1].plot(history.history['val_loss'])\n",
    "      axes[1].plot(history2.history['val_loss'])\n",
    "      axes[1].plot(history3.history['val_loss'])\n",
    "      axes[1].plot(history4.history['val_loss'])\n",
    "      axes[1].plot(history5.history['val_loss'])\n",
    "      axes[1].legend(['Adam', 'Adam_Amsgrad', 'Adadelta', 'SGD', 'Nadam'], loc='upper right')\n",
    "\n",
    "    except:\n",
    "      pass\n",
    "    axes[1].set_title('Validation Loss Vs. No of Epochs for Various Optimizers')\n",
    "    axes[1].set_ylabel('Validation Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "  else:\n",
    "    plt.plot(history.history['loss'])\n",
    "    try:\n",
    "      plt.plot(history.history['val_loss'])\n",
    "      plt.legend(['Train', 'Val'])\n",
    "    except:\n",
    "      pass\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "  \n",
    "plot_val_history(descriptor_history_adam, descriptor_history_adam_amsgrad, descriptor_history_adadelta, descriptor_history_sgd, descriptor_history_nadam, 'mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "improvedupon_code.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/MatchLab-Imperial/keras_triplet_descriptor/blob/master/Baseline_code.ipynb",
     "timestamp": 1552754449861
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
