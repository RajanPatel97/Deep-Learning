{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12807,
     "status": "ok",
     "timestamp": 1552773429425,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "ZZG4BqkENEyd",
    "outputId": "0b8bd808-eba5-4de2-9c01-64058b721fba"
   },
   "outputs": [],
   "source": [
    "# Taken from\n",
    "# https://stackoverflow.com/questions/48750199/google-colaboratory-misleading-information-about-its-gpu-only-5-ram-available\n",
    "# memory footprint support libraries/code\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "# Colab only provides one GPU and it is not always guaranteed\n",
    "gpu = GPUs[0]\n",
    "def printm():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(\"RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2475,
     "status": "ok",
     "timestamp": 1552773429426,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "BBvIvBoyg68g",
    "outputId": "64e90310-ddee-4cb4-a672-41a831cc0503"
   },
   "outputs": [],
   "source": [
    "printm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4124,
     "status": "ok",
     "timestamp": 1552773455053,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "yV1m-9ZGuKGj",
    "outputId": "32fde251-f5a5-4b68-9b12-9e42974bffae"
   },
   "outputs": [],
   "source": [
    "# Clone repo\n",
    "!git clone https://github.com/MatchLab-Imperial/keras_triplet_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 978,
     "status": "ok",
     "timestamp": 1552773459405,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "pyZSqhZ5LACT",
    "outputId": "ce0a5f2e-73e9-43f0-f80b-41950680731a"
   },
   "outputs": [],
   "source": [
    "# Change directory\n",
    "%cd keras_triplet_descriptor    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 190209,
     "status": "ok",
     "timestamp": 1552773652267,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "307CBCL-FjX4",
    "outputId": "9b10a27f-fe9c-4492-a089-784596f08ac0"
   },
   "outputs": [],
   "source": [
    "# Download data\n",
    "!wget -O hpatches_data.zip https://imperialcollegelondon.box.com/shared/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 249292,
     "status": "ok",
     "timestamp": 1552773724185,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "36mBTFvPCxY9",
    "outputId": "b0b44eeb-fe9c-4c11-e2e1-8a67725abd68"
   },
   "outputs": [],
   "source": [
    "# Extract data\n",
    "!unzip -q ./hpatches_data.zip\n",
    "!rm ./hpatches_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0KYfe-at9KN"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda, Reshape\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Conv2DTranspose\n",
    "from keras.layers import Input, UpSampling2D, concatenate  \n",
    "\n",
    "from read_data import HPatches, DataGeneratorDesc, hpatches_sequence_folder, DenoiseHPatches, tps\n",
    "from utils import generate_desc_csv, plot_denoise, plot_triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NXL31ez-AT5h"
   },
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ABKDHB9RApZk"
   },
   "outputs": [],
   "source": [
    "hpatches_dir = './hpatches'\n",
    "splits_path = 'splits.json'\n",
    "\n",
    "splits_json = json.load(open(splits_path, 'rb'))\n",
    "split = splits_json['a']\n",
    "\n",
    "train_fnames = split['train']\n",
    "test_fnames = split['test']\n",
    "\n",
    "seqs = glob.glob(hpatches_dir+'/*')\n",
    "seqs = [os.path.abspath(p) for p in seqs]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs_train = list(filter(lambda x: x.split('\\\\')[-1] in train_fnames, seqs)) \n",
    "seqs_test = list(filter(lambda x: x.split('\\\\')[-1] in split['test'], seqs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(seqs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(seqs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qeWik0vMEtuC"
   },
   "source": [
    "## Models and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6QbkHnbuIUD"
   },
   "outputs": [],
   "source": [
    "def get_denoise_model(shape, do = 0, activate = 'selu'):\n",
    "  \n",
    "    inputs = Input(shape)\n",
    "\n",
    "    ## Encoder starts\n",
    "    conv1 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    ## Bottleneck\n",
    "    conv2 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "\n",
    "    ## Now the decoder starts\n",
    "    up3 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv2))\n",
    "    merge3 = concatenate([conv1,up3], axis = -1)\n",
    "    conv3 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge3)\n",
    "\n",
    "    conv4 = Conv2D(1, 3,  padding = 'same')(conv3)\n",
    "\n",
    "    shallow_net = Model(inputs = inputs, outputs = conv4)\n",
    "\n",
    "    return shallow_net\n",
    "\n",
    "\n",
    "\n",
    "def get_descriptor_model(shape, activate= 'relu'):\n",
    "  \n",
    "    '''Architecture copies HardNet architecture'''\n",
    "\n",
    "    init_weights = keras.initializers.he_normal()\n",
    "\n",
    "    descriptor_model = Sequential()\n",
    "    descriptor_model.add(Conv2D(32, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights))\n",
    "    descriptor_model.add(BatchNormalization(axis = -1))\n",
    "    descriptor_model.add(Activation(activate))\n",
    "\n",
    "    descriptor_model.add(Conv2D(32, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
    "    descriptor_model.add(BatchNormalization(axis = -1))\n",
    "    descriptor_model.add(Activation(activate))\n",
    "\n",
    "    descriptor_model.add(Conv2D(64, 3, padding='same', strides=2, use_bias = True, kernel_initializer=init_weights))\n",
    "    descriptor_model.add(BatchNormalization(axis = -1))\n",
    "    descriptor_model.add(Activation(activate))\n",
    "\n",
    "    descriptor_model.add(Conv2D(64, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
    "    descriptor_model.add(BatchNormalization(axis = -1))\n",
    "    descriptor_model.add(Activation(activate))\n",
    "\n",
    "    descriptor_model.add(Conv2D(128, 3, padding='same', strides=2,  use_bias = True, kernel_initializer=init_weights))\n",
    "    descriptor_model.add(BatchNormalization(axis = -1))\n",
    "    descriptor_model.add(Activation(activate))\n",
    "\n",
    "    descriptor_model.add(Conv2D(128, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
    "    descriptor_model.add(BatchNormalization(axis = -1))\n",
    "    descriptor_model.add(Activation(activate))\n",
    "    descriptor_model.add(Dropout(0.3))\n",
    "\n",
    "    descriptor_model.add(Conv2D(128, 8, padding='valid', use_bias = True, kernel_initializer=init_weights))\n",
    "\n",
    "    # Final descriptor reshape\n",
    "    descriptor_model.add(Reshape((128,)))\n",
    "\n",
    "    return descriptor_model\n",
    "  \n",
    "\n",
    "def triplet_loss(x):\n",
    "  \n",
    "    output_dim = 128\n",
    "    a, p, n = x\n",
    "    _alpha = 1.0\n",
    "    positive_distance = K.mean(K.square(a - p), axis=-1)\n",
    "    negative_distance = K.mean(K.square(a - n), axis=-1)\n",
    "\n",
    "    return K.expand_dims(K.maximum(0.0, positive_distance - negative_distance + _alpha), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RlS5zcV7EJgp"
   },
   "source": [
    "## Denoising Image Patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1293,
     "status": "ok",
     "timestamp": 1552773922597,
     "user": {
      "displayName": "Rajan Patel",
      "photoUrl": "",
      "userId": "15717585103977390251"
     },
     "user_tz": 0
    },
    "id": "-eUSba93Dttj",
    "outputId": "9888dc14-c015-469b-f13c-d1576f667cbb"
   },
   "outputs": [],
   "source": [
    "from keras.layers import LeakyReLU\n",
    "shape = (32, 32, 1)\n",
    "denoise_model = keras.models.load_model('./denoise_base.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vary Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "DVmDZIRTHPDa",
    "outputId": "94bda2ad-e40e-4c44-e74f-be30d956eed6"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Lambda\n",
    "shape = (32, 32, 1)\n",
    "xa = Input(shape=shape, name='a')\n",
    "xp = Input(shape=shape, name='p')\n",
    "xn = Input(shape=shape, name='n')\n",
    "\n",
    "descriptor_model = get_descriptor_model( shape)\n",
    "ea = descriptor_model(xa)\n",
    "ep = descriptor_model(xp)\n",
    "en = descriptor_model(xn)\n",
    "loss = Lambda(triplet_loss)([ea, ep, en])\n",
    "\n",
    "\n",
    "sgd1 = keras.optimizers.SGD(lr=0.00001, momentum=0.9, nesterov=True)\n",
    "sgd2 = keras.optimizers.SGD(lr=0.0001, momentum=0.9, nesterov=True)\n",
    "sgd3 = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
    "sgd4 = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "sgd5 = keras.optimizers.SGD(lr=0.1, momentum=0.9, nesterov=True)\n",
    "\n",
    "descriptor_model_trip_sgd1 = Model(inputs=[xa, xp, xn], outputs=loss)\n",
    "descriptor_model_trip_sgd2 = Model(inputs=[xa, xp, xn], outputs=loss)\n",
    "descriptor_model_trip_sgd3 = Model(inputs=[xa, xp, xn], outputs=loss)\n",
    "descriptor_model_trip_sgd4 = Model(inputs=[xa, xp, xn], outputs=loss)\n",
    "descriptor_model_trip_sgd5 = Model(inputs=[xa, xp, xn], outputs=loss)\n",
    "\n",
    "descriptor_model_trip_sgd1.compile(loss='mean_absolute_error', optimizer=sgd1)\n",
    "descriptor_model_trip_sgd2.compile(loss='mean_absolute_error', optimizer=sgd2)\n",
    "descriptor_model_trip_sgd3.compile(loss='mean_absolute_error', optimizer=sgd3)\n",
    "descriptor_model_trip_sgd4.compile(loss='mean_absolute_error', optimizer=sgd4)\n",
    "descriptor_model_trip_sgd5.compile(loss='mean_absolute_error', optimizer=sgd5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "YIR1cH4fDwKj",
    "outputId": "b17c0b36-e90e-4df0-e969-0ab13136786d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Descriptor loading and training\n",
    "# Loading images\n",
    "hPatches = HPatches(train_fnames=train_fnames, test_fnames=test_fnames,\n",
    "                    denoise_model=denoise_model, use_clean=False)\n",
    "\n",
    "# Creating training generator\n",
    "training_generator = DataGeneratorDesc(*hPatches.read_image_file(hpatches_dir, train=1), num_triplets=10000)\n",
    "# Creating validation generator\n",
    "val_generator = DataGeneratorDesc(*hPatches.read_image_file(hpatches_dir, train=0), num_triplets=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "colab_type": "code",
    "id": "3RQmOMU92csu",
    "outputId": "648c64db-a797-4bb6-c7de-5eb45d27da25"
   },
   "outputs": [],
   "source": [
    "plot_triplet(training_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "QPyc8as42WTQ",
    "outputId": "69ea09ee-8198-42d5-a654-2bd3e339e5b2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#epochs = 1\n",
    "### As with the denoising model, we use a loop to save for each epoch \n",
    "## #the weights in an external website in case colab stops. \n",
    "### reset, so e.g. calling 5 times fit(epochs=1) behave as fit(epochs=5)\n",
    "\n",
    "### If you have a model saved from a previous training session\n",
    "### Load it in the next line\n",
    "# descriptor_model_trip.set_weights(keras.models.load_model('./descriptor.h5').get_weights())\n",
    "# descriptor_model_trip.optimizer = keras.models.load_model('./descriptor.h5').optimizer\n",
    "\n",
    "#for e in range(epochs):\n",
    "  \n",
    "descriptor_history_sgd1 = descriptor_model_trip_sgd1.fit_generator(generator=training_generator, epochs=5, verbose=1, validation_data=val_generator)\n",
    "\n",
    "descriptor_history_sgd2 = descriptor_model_trip_sgd2.fit_generator(generator=training_generator, epochs=5, verbose=1, validation_data=val_generator)\n",
    "\n",
    "descriptor_history_sgd3 = descriptor_model_trip_sgd3.fit_generator(generator=training_generator, epochs=5, verbose=1, validation_data=val_generator)\n",
    "\n",
    "descriptor_history_sgd4 = descriptor_model_trip_sgd4.fit_generator(generator=training_generator, epochs=5, verbose=1, validation_data=val_generator)\n",
    "\n",
    "descriptor_history_sgd5 = descriptor_model_trip_sgd5.fit_generator(generator=training_generator, epochs=5, verbose=1, validation_data=val_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XFA_8uN4Eb3B"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, history2, history3, history4, history5, metric = None):\n",
    "  # Plots the loss history of training and validation (if existing)\n",
    "  # and a given metric\n",
    "  \n",
    "  if metric != None:\n",
    "    fig, axes = plt.subplots(2,1, figsize=(8, 10))\n",
    "    #axes[0].plot(history.history[metric])\n",
    "    #axes[0].plot(history2.history[metric])\n",
    "    #axes[0].plot(history3.history[metric])\n",
    "    #axes[0].plot(history4.history[metric])\n",
    "    #axes[0].plot(history5.history[metric])\n",
    "    #axes[0].plot(history6.history[metric])\n",
    "    try:\n",
    "      #axes[0].plot(history.history['val_'+metric])\n",
    "      #axes[0].plot(history2.history['val2_'+metric])\n",
    "      #axes[0].plot(history3.history['val3_'+metric])\n",
    "      axes[0].legend(['lr=1e-5', 'lr=1e-4', 'lr=1e-3', 'lr=1e-2', 'lr=1e-1'], loc='upper right')\n",
    "    except:\n",
    "      pass\n",
    "    axes[0].set_title('MAE Vs. No of Epochs for Various Learning Rates')\n",
    "    axes[0].set_ylabel('Mean Absolute Error')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    axes[1].plot(history.history['loss'])\n",
    "    axes[1].plot(history2.history['loss'])\n",
    "    axes[1].plot(history3.history['loss'])\n",
    "    axes[1].plot(history4.history['loss'])\n",
    "    axes[1].plot(history5.history['loss'])\n",
    "    try:\n",
    "      #axes[1].plot(history.history['val_loss'])\n",
    "      axes[1].legend(['lr=1e-5', 'lr=1e-4', 'lr=1e-3', 'lr=1e-2', 'lr=1e-1'], loc='upper right')\n",
    "    except:\n",
    "      pass\n",
    "    axes[1].set_title('MAE Vs. No of Epochs for Various Learning Rates')\n",
    "    axes[1].set_ylabel('Mean Absolute Error')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "  else:\n",
    "    plt.plot(history.history['loss'])\n",
    "    try:\n",
    "      plt.plot(history.history['val_loss'])\n",
    "      plt.legend(['Train', 'Val'])\n",
    "    except:\n",
    "      pass\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "  \n",
    "plot_history(descriptor_history_sgd1, descriptor_history_sgd2, descriptor_history_sgd3, descriptor_history_sgd4, descriptor_history_sgd5, 'mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_val_history(history, history2, history3, history4, history5, metric = None):\n",
    "  # Plots the loss history of training and validation (if existing)\n",
    "  # and a given metric\n",
    "  \n",
    "  if metric != None:\n",
    "    fig, axes = plt.subplots(2,1, figsize=(8, 10))\n",
    "    #axes[0].plot(history.history[metric])\n",
    "    #axes[0].plot(history2.history[metric])\n",
    "    #axes[0].plot(history3.history[metric])\n",
    "    try:\n",
    "      #axes[0].plot(history.history['val_'+metric])\n",
    "      #axes[0].plot(history2.history['val_'+metric])\n",
    "      #axes[0].plot(history3.history['val_'+metric])\n",
    "      #axes[0].plot(history4.history['val_'+metric])\n",
    "      #axes[0].plot(history5.history['val_'+metric])\n",
    "      #axes[0].plot(history6.history['val_'+metric])\n",
    "      axes[0].legend(['lr=1e-5', 'lr=1e-4', 'lr=1e-3', 'lr=1e-2', 'lr=1e-1'], loc='upper right')\n",
    "\n",
    "    except:\n",
    "      pass\n",
    "    axes[0].set_title('Validation Loss Vs. No of Epochs for Various Learning Rates')\n",
    "    axes[0].set_ylabel('Validation Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    #axes[1].plot(history.history['loss'])\n",
    "    #axes[1].plot(history2.history['loss'])\n",
    "    #axes[1].plot(history3.history['loss'])\n",
    "    try:\n",
    "      axes[1].plot(history.history['val_loss'])\n",
    "      axes[1].plot(history2.history['val_loss'])\n",
    "      axes[1].plot(history3.history['val_loss'])\n",
    "      axes[1].plot(history4.history['val_loss'])\n",
    "      axes[1].plot(history5.history['val_loss'])\n",
    "      #axes[1].plot(history6.history['val_loss'])\n",
    "      axes[1].legend(['lr=1e-5', 'lr=1e-4', 'lr=1e-3', 'lr=1e-2', 'lr=1e-1'], loc='upper right')\n",
    "\n",
    "    except:\n",
    "      pass\n",
    "    axes[1].set_title('Validation Loss Vs. No of Epochs for Various Learning Rates')\n",
    "    axes[1].set_ylabel('Validation Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "  else:\n",
    "    plt.plot(history.history['loss'])\n",
    "    try:\n",
    "      plt.plot(history.history['val_loss'])\n",
    "      plt.legend(['Train', 'Val'])\n",
    "    except:\n",
    "      pass\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "  \n",
    "plot_val_history(descriptor_history_sgd1, descriptor_history_sgd2, descriptor_history_sgd3, descriptor_history_sgd4, descriptor_history_sgd5, 'mean_absolute_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vary Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd1 = keras.optimizers.SGD(lr=0.1, momentum=0.9, nesterov=True)\n",
    "sgd2 = keras.optimizers.SGD(lr=0.1, momentum=0.8, nesterov=True)\n",
    "sgd3 = keras.optimizers.SGD(lr=0.1, momentum=0.7, nesterov=True)\n",
    "sgd4 = keras.optimizers.SGD(lr=0.1, momentum=0.6, nesterov=True)\n",
    "sgd5 = keras.optimizers.SGD(lr=0.1, momentum=0.5, nesterov=True)\n",
    "\n",
    "descriptor_model_trip_sgd1 = Model(inputs=[xa, xp, xn], outputs=loss)\n",
    "descriptor_model_trip_sgd2 = Model(inputs=[xa, xp, xn], outputs=loss)\n",
    "descriptor_model_trip_sgd3 = Model(inputs=[xa, xp, xn], outputs=loss)\n",
    "descriptor_model_trip_sgd4 = Model(inputs=[xa, xp, xn], outputs=loss)\n",
    "descriptor_model_trip_sgd5 = Model(inputs=[xa, xp, xn], outputs=loss)\n",
    "\n",
    "descriptor_model_trip_sgd1.compile(loss='mean_absolute_error', optimizer=sgd1)\n",
    "descriptor_model_trip_sgd2.compile(loss='mean_absolute_error', optimizer=sgd2)\n",
    "descriptor_model_trip_sgd3.compile(loss='mean_absolute_error', optimizer=sgd3)\n",
    "descriptor_model_trip_sgd4.compile(loss='mean_absolute_error', optimizer=sgd4)\n",
    "descriptor_model_trip_sgd5.compile(loss='mean_absolute_error', optimizer=sgd5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epochs = 1\n",
    "### As with the denoising model, we use a loop to save for each epoch \n",
    "## #the weights in an external website in case colab stops. \n",
    "### reset, so e.g. calling 5 times fit(epochs=1) behave as fit(epochs=5)\n",
    "\n",
    "### If you have a model saved from a previous training session\n",
    "### Load it in the next line\n",
    "# descriptor_model_trip.set_weights(keras.models.load_model('./descriptor.h5').get_weights())\n",
    "# descriptor_model_trip.optimizer = keras.models.load_model('./descriptor.h5').optimizer\n",
    "\n",
    "#for e in range(epochs):\n",
    "  \n",
    "descriptor_history_sgd1 = descriptor_model_trip_sgd1.fit_generator(generator=training_generator, epochs=5, verbose=1, validation_data=val_generator)\n",
    "\n",
    "descriptor_history_sgd2 = descriptor_model_trip_sgd2.fit_generator(generator=training_generator, epochs=5, verbose=1, validation_data=val_generator)\n",
    "\n",
    "descriptor_history_sgd3 = descriptor_model_trip_sgd3.fit_generator(generator=training_generator, epochs=5, verbose=1, validation_data=val_generator)\n",
    "\n",
    "descriptor_history_sgd4 = descriptor_model_trip_sgd4.fit_generator(generator=training_generator, epochs=5, verbose=1, validation_data=val_generator)\n",
    "\n",
    "descriptor_history_sgd5 = descriptor_model_trip_sgd5.fit_generator(generator=training_generator, epochs=5, verbose=1, validation_data=val_generator)\n",
    "\n",
    "\n",
    "### Saves optimizer and weights\n",
    "#descriptor_model_trip.save('descriptor.h5') \n",
    "### Uploads files to external hosting\n",
    "#!curl -F \"file=@descriptor.h5\" https://file.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, history2, history3, history4, history5, metric = None):\n",
    "  # Plots the loss history of training and validation (if existing)\n",
    "  # and a given metric\n",
    "  \n",
    "  if metric != None:\n",
    "    fig, axes = plt.subplots(2,1, figsize=(8, 10))\n",
    "    #axes[0].plot(history.history[metric])\n",
    "    #axes[0].plot(history2.history[metric])\n",
    "    #axes[0].plot(history3.history[metric])\n",
    "    #axes[0].plot(history4.history[metric])\n",
    "    #axes[0].plot(history5.history[metric])\n",
    "    \n",
    "    try:\n",
    "      #axes[0].plot(history.history['val_'+metric])\n",
    "      #axes[0].plot(history2.history['val2_'+metric])\n",
    "      #axes[0].plot(history3.history['val3_'+metric])\n",
    "      axes[0].legend(['0.9', '0.8', '0.7', '0.6', '0.5'], loc='best')\n",
    "    except:\n",
    "      pass\n",
    "    axes[0].set_title('MAE Vs. No of Epochs for Various Momentum Values')\n",
    "    axes[0].set_ylabel('Mean Absolute Error')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    axes[1].plot(history.history['loss'])\n",
    "    axes[1].plot(history2.history['loss'])\n",
    "    axes[1].plot(history3.history['loss'])\n",
    "    axes[1].plot(history4.history['loss'])\n",
    "    axes[1].plot(history5.history['loss'])\n",
    "    try:\n",
    "      #axes[1].plot(history.history['val_loss'])\n",
    "      axes[1].legend(['0.9', '0.8', '0.7', '0.6', '0.5'], loc='best')\n",
    "    except:\n",
    "      pass\n",
    "    axes[1].set_title('MAE Vs. No of Epochs for Various Momentum Values')\n",
    "    axes[1].set_ylabel('Mean Absolute Error')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "  else:\n",
    "    plt.plot(history.history['loss'])\n",
    "    try:\n",
    "      plt.plot(history.history['val_loss'])\n",
    "      plt.legend(['Train', 'Val'])\n",
    "    except:\n",
    "      pass\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "  \n",
    "plot_history(descriptor_history_sgd1, descriptor_history_sgd2, descriptor_history_sgd3, descriptor_history_sgd4, descriptor_history_sgd5, 'mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_val_history(history, history2, history3, history4, history5, metric = None):\n",
    "  # Plots the loss history of training and validation (if existing)\n",
    "  # and a given metric\n",
    "  \n",
    "  if metric != None:\n",
    "    fig, axes = plt.subplots(2,1, figsize=(8, 10))\n",
    "    #axes[0].plot(history.history[metric])\n",
    "    #axes[0].plot(history2.history[metric])\n",
    "    #axes[0].plot(history3.history[metric])\n",
    "    try:\n",
    "      #axes[0].plot(history.history['val_'+metric])\n",
    "      #axes[0].plot(history2.history['val_'+metric])\n",
    "      #axes[0].plot(history3.history['val_'+metric])\n",
    "      #axes[0].plot(history4.history['val_'+metric])\n",
    "      #axes[0].plot(history5.history['val_'+metric])\n",
    "      axes[0].legend(['0.9', '0.8', '0.7', '0.6', '0.5'], loc='best')\n",
    "    except:\n",
    "      pass\n",
    "    axes[0].set_title('Validation Loss Vs. No of Epochs for for Various Momentum Values')\n",
    "    axes[0].set_ylabel('Validation Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    #axes[1].plot(history.history['loss'])\n",
    "    #axes[1].plot(history2.history['loss'])\n",
    "    #axes[1].plot(history3.history['loss'])\n",
    "    try:\n",
    "      axes[1].plot(history.history['val_loss'])\n",
    "      axes[1].plot(history2.history['val_loss'])\n",
    "      axes[1].plot(history3.history['val_loss'])\n",
    "      axes[1].plot(history4.history['val_loss'])\n",
    "      axes[1].plot(history5.history['val_loss'])\n",
    "      axes[1].legend(['0.9', '0.8', '0.7', '0.6', '0.5'], loc='best')\n",
    "    except:\n",
    "      pass\n",
    "    axes[1].set_title('Validation Loss Vs. No of Epochs for Various Momentum Values')\n",
    "    axes[1].set_ylabel('Validation Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "  else:\n",
    "    plt.plot(history.history['loss'])\n",
    "    try:\n",
    "      plt.plot(history.history['val_loss'])\n",
    "      plt.legend(['Train', 'Val'])\n",
    "    except:\n",
    "      pass\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    \n",
    "plot_val_history(descriptor_history_sgd1, descriptor_history_sgd2, descriptor_history_sgd3, descriptor_history_sgd4, descriptor_history_sgd5, 'mean_absolute_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd1 = keras.optimizers.SGD(lr=0.1, momentum=0.7, nesterov=True)\n",
    "\n",
    "descriptor_model_trip = Model(inputs=[xa, xp, xn], outputs=loss)\n",
    "\n",
    "descriptor_model_trip.compile(loss='mean_absolute_error', optimizer=sgd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#epochs = 1\n",
    "### As with the denoising model, we use a loop to save for each epoch \n",
    "## #the weights in an external website in case colab stops. \n",
    "### reset, so e.g. calling 5 times fit(epochs=1) behave as fit(epochs=5)\n",
    "\n",
    "### If you have a model saved from a previous training session\n",
    "### Load it in the next line\n",
    "# descriptor_model_trip.set_weights(keras.models.load_model('./descriptor.h5').get_weights())\n",
    "# descriptor_model_trip.optimizer = keras.models.load_model('./descriptor.h5').optimizer\n",
    "\n",
    "#for e in range(epochs):\n",
    "  \n",
    "descriptor_history = descriptor_model_trip.fit_generator(generator=training_generator, epochs=20, verbose=1, validation_data=val_generator)\n",
    "descriptor_model_trip.save('descriptor_base.h5') \n",
    "\n",
    "### Saves optimizer and weights\n",
    "#descriptor_model_trip.save('descriptor.h5') \n",
    "### Uploads files to external hosting\n",
    "#!curl -F \"file=@descriptor.h5\" https://file.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test mAP on Baseline Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate_desc_csv(descriptor_model, seqs_test, denoise_model=denoise_model, use_clean=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/keras_triplet_descriptor/out/ --task=verification --delimiter=\";\"\n",
    "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/keras_triplet_descriptor/out/ --task=matching --delimiter=\";\"\n",
    "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/keras_triplet_descriptor/out/ --task=retrieval --delimiter=\";\"\n",
    "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=retrieval"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "improvedupon_code.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/MatchLab-Imperial/keras_triplet_descriptor/blob/master/Baseline_code.ipynb",
     "timestamp": 1552754449861
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
